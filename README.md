## Hi there üëã

I am an upcoming joint **PhD student** at **the State Key Laboratory of General Artificial Intelligence** <a href='https://www.bigai.ai/'>(BIGAI)</a> and **the University of Science and Technology of China** (USTC), supervised by <a href='https://liqing.io/'>Qing Li(BIGAI, ÊùéÂ∫Ü)</a>, <a href='https://yuntaodu.github.io/'>Yuntao Du(SDU, Êùú‰∫ëÊ∂õ)</a>, <a href='https://siyuanqi.github.io/'>Siyuan Qi (Gyges Labs, Á∂¶ÊÄùÊ∫ê)</a> and <a href='http://staff.ustc.edu.cn/~binli/'>Bin Li (USTC, ÊùéÊñå)</a>, <a href='https://faculty.ustc.edu.cn/liulei13/zh_CN/index.htm'>Lei Liu (USTC, ÂàòÁ£ä)</a>. Currently, I am doing my internship in State Key Laboratory of General Artificial Intelligence.

I am currently working on knowledge editing, knowledge injection, multimodal learning, continual learning, <a href='https://scholar.google.com/citations?user=NSHQsrAAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FKailinJiang%2Fkailinjiang.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations">

My huggingface at ü§ó [Huggingface home](https://huggingface.co/kailinjiang).

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# üî• News
<!-- Allowed emojis: üéâüéâfor good news üì£üì£for average news-->
- **2025.03**: &nbsp;üéâüéâ One paper have been accepted by **ICLR 2025 Workshop SSI-FM**! <a href='https://mmke-bench-iclr.github.io/'>Multimodal Knowledge Editing</a>ÔºÅ
- **2025.01**: &nbsp;üéâüéâ Two paper have been accepted by **ICLR 2025**! <a href='https://arxiv.org/pdf/2406.11194'>In Context Editing</a> and <a href='https://mmke-bench-iclr.github.io/'>Multimodal Knowledge Editing</a>ÔºÅ
- **2024.06**: &nbsp;üéâüéâ I successfully completed my undergraduate studies from the College of Science of Sichuan Agricultural University!
- **2024.02**: &nbsp;üì£üì£ I will go to the State Key Laboratory of General Artificial Intelligence <a href='https://www.bigai.ai/'>(BIGAI)</a> to start my internship!

### üìé Homepages
- Personal Pages: üå± [Personal Pages](https://kailinjiang.github.io/). (updated recentlyüî•)
- Google Scholar: üî≠ [Google Scholar](https://scholar.google.com/citations?user=NSHQsrAAAAAJ&hl=zh-CN). 
- BIGAI EmailÔºöüì´  jiangkailin@bigai.ai
- USTC EmailÔºöüì´  kailinjiang@mail.ustc.edu.cn

# üìù Publications
*: Co-First Author, Equal Contribution

- `ICLR2025 & ICLR2025 Workshop SSI-FM` [MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge](https://arxiv.org/abs/2502.19870), Yuntao Du\*, **Kailin Jiang\***, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li. „Äê2024.10„Äë<br>
<b style="color: #8B0000;">The Thirteenth International Conference on Learning Representations</b>


[![arXiv](https://img.shields.io/badge/Arxiv-2502.19870-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2502.19870) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-MMKE_Bench-blue)](https://huggingface.co/datasets/kailinjiang/MMKE-Bench-dataset)  [![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Model-MMKE_Bench-blue)](https://huggingface.co/kailinjiang/MMKE-Bench) [![code](https://img.shields.io/badge/Code-MMKE_Bench-blue?logo=github)](https://github.com/MMKE-Bench-ICLR/MMKE-Bench) [![website](https://img.shields.io/badge/Website-MMKE_Bench-orange?logo=homepage)](https://mmke-bench-iclr.github.io/) [![depositphotos](https://img.shields.io/badge/Poster-MMKE_Bench-red?logo=depositphotos)](./images/poster/iclr25_mmke_bench_poster.pdf) [![Slides](https://img.shields.io/badge/%F0%9F%93%8A%20Slides-MMKE_Bench-BF55EC)](https://mmke-bench-iclr.github.io/static/Slides/MMKE-Bench.pdf)


[![airchina](https://img.shields.io/badge/Êï∞Ê∫êAI-MMKE_Bench-red?logo=airchina)](https://mp.weixin.qq.com/s/iN826lITi5Xyz-3GnrdVIQ) [![codeproject](https://img.shields.io/badge/ÈáèÂ≠ê‰πãÂøÉ-MMKE_Bench-red?logo=codeproject)](https://www.xiaohongshu.com/explore/67e2d622000000000603cbfc?note_flow_source=wechat&xsec_token=CBldN8wUavDAzFvP4tK_noXO94RAXcelKKqlO3pFiJ6EQ=) [![actix](https://img.shields.io/badge/ÊûÅÂ∏ÇÂπ≥Âè∞-MMKE_Bench-red?logo=actix)](https://mp.weixin.qq.com/s/JfxeytzWU0QoIUfJTGqgQQ) [![zhihu](https://img.shields.io/badge/Áü•‰πé-MMKE_Bench-red?logo=zhihu)](https://zhuanlan.zhihu.com/p/30599722521) [![tong](https://img.shields.io/badge/ÈÄöÊô∫Â∞ëÂπ¥-MMKE_Bench-red?logo=wechat)](https://mp.weixin.qq.com/s/B4eL3sG3TI63i3imeKdw1w)



- `ICLR2025` [In-Context Editing: Learning Knowledge from Self-Induced Distributions](https://arxiv.org/pdf/2406.11194), Siyuan Qi\*, Bangcheng Yang\*, **Kailin Jiang\***, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, Zilong Zheng. „Äê2024.06„Äë<br>
<b style="color: #8B0000;">The Thirteenth International Conference on Learning Representations</b>


[![arXiv](https://img.shields.io/badge/Arxiv-2406.11194-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2406.11194)  [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-ICE-blue)](https://huggingface.co/datasets/Yofuria/ICE)    [![code](https://img.shields.io/badge/Code-ICE-blue?logo=github)](https://github.com/bigai-ai/ICE) [![depositphotos](https://img.shields.io/badge/Poster-ICE-red?logo=depositphotos)](./images/poster/ICE_poster.png)

[![AIModels.fyi](https://img.shields.io/badge/AIModels.fyi-ICE-blue?logo=anthropic)](https://www.aimodels.fyi/papers/arxiv/context-editing-learning-knowledge-from-self-induced) [![actix](https://img.shields.io/badge/ÊûÅÂ∏ÇÂπ≥Âè∞-ICE-red?logo=actix)](https://mp.weixin.qq.com/s/Mr9HPeHJSsVfUIeF6j-zWw)





# üìù Preprints
*: Co-First Author, Equal Contribution



- `Preprint` [When Large Multimodal Models Confront Evolving Knowledge: Challenges and Pathways](https://arxiv.org/abs/2505.24449),  **Kailin Jiang\***, Yuntao Du\*, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, Qing Li.„Äê2025.3„Äë<br>

[![arXiv](https://img.shields.io/badge/Arxiv-2505.24449-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2505.24449) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-EVOKE-blue)](https://huggingface.co/datasets/kailinjiang/EVOKE)   [![code](https://img.shields.io/badge/Code-EVOKE-blue?logo=github)](https://github.com/EVOKE-LMM/EVOKE)  [![website](https://img.shields.io/badge/Website-EVOKE-orange?logo=homepage)](https://evoke-lmm.github.io/) [![Slides](https://img.shields.io/badge/%F0%9F%93%8A%20Slides-EVOKE-BF55EC)](https://evoke-lmm.github.io/EVOKE/slides/When%20Large%20Multimodal%20Models%20Confront%20Evolving%20Knowledge%20Challenges%20and%20Pathways.pdf)

- `Preprint` [MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models](https://arxiv.org/pdf/2510.19457), **Kailin Jiang\***, Ning Jiang\*, Yuntao Du\*, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu. „Äê2025.5„Äë<br>

[![arXiv](https://img.shields.io/badge/Arxiv-2510.19457-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2510.19457) [![website](https://img.shields.io/badge/Website-MINED-orange?logo=homepage)](https://mined-lmm.github.io/) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-MINED-blue)](https://huggingface.co/datasets/kailinjiang/MINED)   [![code](https://img.shields.io/badge/Code-MINED-blue?logo=github)](https://github.com/MINED-LMM/MINED) [![Slides](https://img.shields.io/badge/%F0%9F%93%8A%20Slides-MINED-BF55EC)](https://mined-lmm.github.io/MINED/MINED.pdf)





- `Preprint` [Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models](https://arxiv.org/pdf/2505.19509), Yifan Jia\*, **Kailin Jiang\***, Yuyang Liang, Qihan Ren, Yi Xin, Rui Yang, Fenze Feng, Mingcai Chen, Hengyang Lu, Haozhe Wang, Xiaoye Qu, Dongrui Liu, Lizhen Cui, Yuntao Du. „Äê2025.5„Äë<br>

[![arXiv](https://img.shields.io/badge/Arxiv-2505.19509-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2505.19509) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-MMKC_Bench-blue)](https://huggingface.co/datasets/starjyf/MLLMKC-datasets)   [![code](https://img.shields.io/badge/Code-MMKC_Bench-blue?logo=github)](https://github.com/MLLMKCBENCH/MLLMKC) [![website](https://img.shields.io/badge/Website-MMKC_Bench-orange?logo=homepage)](https://mllmkcbench.github.io/)

[![airchina](https://img.shields.io/badge/Êï∞Ê∫êAI-MMKC_Bench-red?logo=airchina)](https://mp.weixin.qq.com/s/nlnFtrWrhFhKaS1vBgOp3Q)


- `Preprint` [KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints](https://arxiv.org/pdf/2510.19316),  **Kailin Jiang**, Hongbo Jiang, Ning Jiang, Zhi Gao, Jinhe Bi, Yuchen Ren, Bin Li, Yuntao Du, Lei Liu, Qing Li.„Äê2025.9„Äë<br>

[![arXiv](https://img.shields.io/badge/Arxiv-2510.19316-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2510.19316) [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-KORE-blue)](https://huggingface.co/datasets/kailinjiang/KORE-74K)  [![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Model-KORE-blue)](https://huggingface.co/collections/kailinjiang/kore-68c54e73b6a19eece0fff381) [![code](https://img.shields.io/badge/Code-KORE-blue?logo=github)](https://github.com/KORE-LMM/KORE)  [![website](https://img.shields.io/badge/Website-KORE-orange?logo=homepage)](https://kore-lmm.github.io/) [![Slides](https://img.shields.io/badge/%F0%9F%93%8A%20Slides-KORE-BF55EC)](https://kore-lmm.github.io/KORE/slides/KORE.pdf)


[![xhs](https://img.shields.io/badge/AiÁáÉÊòüÁêÉüî•üåè-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/discovery/item/6902ca730000000004007ba3?app_platform=android&ignoreEngage=true&app_version=9.6.0&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBbLFXgqEfLsfhcBN4HXZPeCyCyGEsBHQe-5dkjpd0RiY%3D&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761792178&share_id=d7bd378804874ad3a7c2fdd58dd4141f&share_channel=wechat) [![xhs](https://img.shields.io/badge/Â∞èÁ∫¢ËñØT163HHJD8-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/explore/69006edd000000000303a091?app_platform=android&ignoreEngage=true&app_version=8.85.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBJqGBgFq9JYyanei2N0lzQqz-FxyTJIQ0hlbuthD3rQA=&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761637080&share_id=f96b1badda154f37bfecdf6814d6c494&share_channel=wechat&wechatWid=50306f7421895f5d50f087a41a373ab0&wechatOrigin=menu) [![xhs](https://img.shields.io/badge/mllm-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/explore/690038560000000005039f84?app_platform=android&ignoreEngage=true&app_version=8.85.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBJqGBgFq9JYyanei2N0lzQoljxr9qdchUP0_eH_HVRJ8=&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761623869&share_id=b1aba7e5634d489f9b21f342aebe62a3&share_channel=wechat&wechatWid=50306f7421895f5d50f087a41a373ab0&wechatOrigin=menu) [![xhs](https://img.shields.io/badge/ËÆ∫ÊñáÈ©øÁ´ô-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/explore/68fed64f00000000030113b7?app_platform=android&ignoreEngage=true&app_version=8.85.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBHAuHgi51U9_ccgfuhEBgml3l7jq0sKf-DOuH8a4HZyc=&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761620477&share_id=4fb9979da8f54838870b7490d2ef5930&share_channel=wechat&wechatWid=50306f7421895f5d50f087a41a373ab0&wechatOrigin=menu) [![xhs](https://img.shields.io/badge/AIÈÄüËØëÂÆò-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/explore/68ff3a3b0000000003013ef0?app_platform=android&ignoreEngage=true&app_version=8.85.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBGA_btiXu5fjjNG3XoH3Cx_Jq1jvk4fy6IUqNASxNw9Q=&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761620457&share_id=978ea196b87742429af836f293de86a7&share_channel=wechat&wechatWid=50306f7421895f5d50f087a41a373ab0&wechatOrigin=menu) [![xhs](https://img.shields.io/badge/ËÆ∫ÊñáÈòÖËØª-KORE-red?logo=xiaohongshu)](https://www.xiaohongshu.com/explore/69002424000000000700c2e4?app_platform=android&ignoreEngage=true&app_version=8.85.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBJqGBgFq9JYyanei2N0lzQhz8YQifKdumpqwpgK4qKro=&author_share=1&xhsshare=WeixinSession&shareRedId=ODdGODg8Skw2NzUyOTgwNjczOTdJOjdO&apptime=1761620422&share_id=be8bf11b7c6c4da585630331bebbc19f&share_channel=wechat&wechatWid=50306f7421895f5d50f087a41a373ab0&wechatOrigin=menu) [![tong](https://img.shields.io/badge/AIÈÄüËØëÂÆò-KORE-red?logo=wechat)](https://mp.weixin.qq.com/s/wOdQKemYV3XJXO-tgW0R_A?scene=1&click_id=4)





# üì∞ Peer Review
- ICLR 2025 Workshop SSI-FM Reviewer
- ICLR 2026 Reviewer


# üéñ Honors and Awards
- **2022.12** Typical Innovation and Entrepreneurship Models of Sichuan Agricultural University (10 students).
- **2022.11** China Telecom Scholarship ¬∑ Fly Young Award.
- **2022.04** Youth May Fourth Model of Sichuan Agricultural University (10 students).
- **2021.11** National First Prize of Undergraduate Group of National Undergraduate Mathematical Modeling Contest of Gaojiaoshe Cup,team leader. 


# üìñ Educations
- **2024.06 - now**, **University of Science and Technology of China (USTC), PhD student**. I am pursuing a degree in Information and Communication Engineering at USTC's School of Information Science and Technology, and the program is co-training with the State Key Laboratory of General Artificial Intelligence.

- **2020.09 - 2024.06**, **Sichuan Agricultural University (SICAU), graduate student**. I am studying for a degree in Information and Computational Science at the college of science in SICAU.


# üíª Internships
- **2024.08 - now**, <img src='./images/logo960.png' style='width: 6em;'> the State Key Laboratory of General Artificial Intelligence(Beijing,China), **ML Lab**, Intern Researcher.
- **2024.02 - 2024.08**, <img src='./images/logo960.png' style='width: 6em;'> the State Key Laboratory of General Artificial Intelligence(Beijing,China), **MAS Lab**, Algorithm Intern.





<p align="center">
  <img src="images/logo960.png" alt="BIGAI" width="200" height="60">
</p>







# üìä GitHub Stats

![Kailin Jiang's GitHub Stats](https://github-readme-stats.vercel.app/api?username=kailinjiang&show_icons=true&theme=tokyonight)







<!--
**kailinjiang/kailinjiang** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
